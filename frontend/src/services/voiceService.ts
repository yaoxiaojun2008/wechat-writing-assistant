import axios from 'axios';
import { ApiResponse } from '../types';

export interface VoiceRecordingOptions {
  sampleRate?: number;
  channelCount?: number;
  echoCancellation?: boolean;
  noiseSuppression?: boolean;
}

export interface TranscriptionResult {
  transcribedText: string;
  timestamp: string;
}

export class VoiceRecordingService {
  private mediaRecorder: MediaRecorder | null = null;
  private audioChunks: Blob[] = [];
  private stream: MediaStream | null = null;
  private isRecording = false;

  private readonly defaultOptions: VoiceRecordingOptions = {
    sampleRate: 44100,
    channelCount: 1,
    echoCancellation: true,
    noiseSuppression: true,
  };

  async startRecording(options?: VoiceRecordingOptions): Promise<void> {
    if (this.isRecording) {
      throw new Error('å½•éŸ³å·²åœ¨è¿›è¡Œä¸­');
    }

    try {
      // Check if browser supports getUserMedia
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³å½•åˆ¶åŠŸèƒ½');
      }

      const constraints: MediaStreamConstraints = {
        audio: {
          ...this.defaultOptions,
          ...options,
        },
      };

      // Request microphone access
      this.stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Initialize MediaRecorder
      const mimeType = this.getSupportedMimeType();
      this.mediaRecorder = new MediaRecorder(this.stream, {
        mimeType,
      });

      // Reset audio chunks
      this.audioChunks = [];

      // Set up event handlers
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data);
        }
      };

      this.mediaRecorder.onerror = (event) => {
        console.error('MediaRecorder error:', event);
        this.cleanup();
        throw new Error('å½•éŸ³è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯');
      };

      // Start recording
      this.mediaRecorder.start(1000); // Collect data every second
      this.isRecording = true;

    } catch (error) {
      this.cleanup();
      if (error instanceof Error) {
        if (error.name === 'NotAllowedError') {
          throw new Error('è¯·å…è®¸è®¿é—®éº¦å…‹é£ä»¥ä½¿ç”¨è¯­éŸ³å½•åˆ¶åŠŸèƒ½');
        } else if (error.name === 'NotFoundError') {
          throw new Error('æœªæ‰¾åˆ°å¯ç”¨çš„éº¦å…‹é£è®¾å¤‡');
        } else if (error.name === 'NotSupportedError') {
          throw new Error('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³å½•åˆ¶åŠŸèƒ½');
        }
        throw error;
      }
      throw new Error('å¯åŠ¨å½•éŸ³å¤±è´¥');
    }
  }

  async stopRecording(): Promise<Blob> {
    if (!this.isRecording || !this.mediaRecorder) {
      throw new Error('å½“å‰æ²¡æœ‰è¿›è¡Œå½•éŸ³');
    }

    return new Promise((resolve, reject) => {
      if (!this.mediaRecorder) {
        reject(new Error('MediaRecorder æœªåˆå§‹åŒ–'));
        return;
      }

      this.mediaRecorder.onstop = () => {
        try {
          const mimeType = this.getSupportedMimeType();
          const audioBlob = new Blob(this.audioChunks, { type: mimeType });
          this.cleanup();
          resolve(audioBlob);
        } catch (error) {
          reject(error);
        }
      };

      this.mediaRecorder.stop();
      this.isRecording = false;
    });
  }

  async transcribeAudio(audioBlob: Blob): Promise<TranscriptionResult> {
    try {
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.wav');

      // Get session token from localStorage
      const sessionId = localStorage.getItem('sessionId');
      const API_BASE_URL = import.meta.env.VITE_API_URL || 'http://localhost:3001/api';
      
      const response = await axios.post<ApiResponse<TranscriptionResult>>(
        `${API_BASE_URL}/voice/transcribe`,
        formData,
        {
          headers: {
            'Content-Type': 'multipart/form-data',
            'Authorization': sessionId ? `Bearer ${sessionId}` : '',
          },
          timeout: 30000, // 30 second timeout
        }
      );

      if (!response.data.success || !response.data.data) {
        throw new Error(response.data.error?.message || 'è¯­éŸ³è½¬æ–‡å­—å¤±è´¥');
      }

      return response.data.data;
    } catch (error) {
      if (axios.isAxiosError(error)) {
        if (error.code === 'ECONNABORTED') {
          throw new Error('è¯­éŸ³å¤„ç†è¶…æ—¶ï¼Œè¯·é‡è¯•');
        } else if (error.response?.status === 413) {
          throw new Error('éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼Œè¯·å½•åˆ¶è¾ƒçŸ­çš„è¯­éŸ³');
        } else if (error.response?.status === 401) {
          throw new Error('è®¤è¯å¤±è´¥ï¼Œè¯·é‡æ–°ç™»å½•');
        } else if (error.response?.data?.error?.message) {
          throw new Error(error.response.data.error.message);
        }
      }
      throw new Error('è¯­éŸ³è½¬æ–‡å­—å¤„ç†å¤±è´¥ï¼Œè¯·é‡è¯•');
    }
  }

  cancelRecording(): void {
    if (this.isRecording && this.mediaRecorder) {
      this.mediaRecorder.stop();
      this.isRecording = false;
    }
    this.cleanup();
  }

  getRecordingState(): boolean {
    return this.isRecording;
  }

  private getSupportedMimeType(): string {
    const types = [
      'audio/webm;codecs=opus',
      'audio/webm',
      'audio/ogg;codecs=opus',
      'audio/ogg',
      'audio/wav',
    ];

    for (const type of types) {
      if (MediaRecorder.isTypeSupported(type)) {
        return type;
      }
    }

    // Fallback to basic webm if nothing else is supported
    return 'audio/webm';
  }

  private cleanup(): void {
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
      this.stream = null;
    }
    
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
  }
}

// Web Speech API service (alternative/fallback)
export class WebSpeechService {
  private recognition: SpeechRecognition | null = null;
  private isListening = false;

  constructor() {
    // Check for Web Speech API support
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (SpeechRecognition) {
      this.recognition = new SpeechRecognition();
      this.setupRecognition();
      console.log('Web Speech API initialized successfully');
    } else {
      console.warn('Web Speech API not supported in this browser');
    }
  }

  private setupRecognition(): void {
    if (!this.recognition) return;

    // Configure for better Chinese speech recognition
    this.recognition.continuous = true; // Enable continuous recognition for longer speech
    this.recognition.interimResults = true;
    this.recognition.lang = 'zh-CN';
    this.recognition.maxAlternatives = 1;
    
    // Additional settings for better performance
    if ('webkitSpeechRecognition' in window) {
      // Chrome-specific optimizations
      (this.recognition as any).serviceURI = 'wss://www.google.com/speech-api/full-duplex/v1/up';
    }
    
    console.log('Web Speech API configured for Chinese (zh-CN) with continuous recognition');
  }

  isSupported(): boolean {
    return this.recognition !== null;
  }

  async startListening(): Promise<string> {
    if (!this.recognition) {
      throw new Error('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½');
    }

    if (this.isListening) {
      throw new Error('è¯­éŸ³è¯†åˆ«å·²åœ¨è¿›è¡Œä¸­');
    }

    return new Promise((resolve, reject) => {
      if (!this.recognition) {
        reject(new Error('è¯­éŸ³è¯†åˆ«æœªåˆå§‹åŒ–'));
        return;
      }

      let finalTranscript = '';
      let interimTranscript = '';
      let timeoutId: NodeJS.Timeout | null = null;
      let hasReceivedSpeech = false;

      this.recognition.onstart = () => {
        this.isListening = true;
        console.log('ğŸ¤ Web Speech API started listening');
        console.log('Configuration:', {
          continuous: this.recognition?.continuous,
          interimResults: this.recognition?.interimResults,
          lang: this.recognition?.lang,
          maxAlternatives: this.recognition?.maxAlternatives
        });
      };

      this.recognition.onresult = (event) => {
        hasReceivedSpeech = true;
        interimTranscript = '';
        
        console.log('ğŸ“ Speech recognition result event:', {
          resultIndex: event.resultIndex,
          resultsLength: event.results.length
        });
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          const confidence = event.results[i][0].confidence;
          
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
            console.log('âœ… Final transcript added:', transcript, 'confidence:', confidence);
            console.log('ğŸ“„ Total final transcript so far:', finalTranscript);
          } else {
            interimTranscript += transcript;
            console.log('â³ Interim transcript:', transcript, 'confidence:', confidence);
          }
        }

        // Reset timeout when we receive speech
        if (timeoutId) {
          clearTimeout(timeoutId);
        }
        // Set a longer timeout for silence detection (8 seconds of silence)
        // This allows for natural pauses in speech
        timeoutId = setTimeout(() => {
          console.log('ğŸ”‡ Extended silence detected, stopping recognition');
          console.log('ğŸ“‹ Final result before stopping:', finalTranscript);
          this.stopListening();
        }, 8000); // Increased from 5 to 8 seconds
      };

      this.recognition.onend = () => {
        this.isListening = false;
        if (timeoutId) {
          clearTimeout(timeoutId);
        }
        
        // Combine final and interim results
        const result = (finalTranscript + interimTranscript).trim();
        console.log('ğŸ Recognition ended');
        console.log('ğŸ“„ Final transcript:', finalTranscript);
        console.log('â³ Interim transcript:', interimTranscript);
        console.log('ğŸ¯ Combined result:', result);
        console.log('ğŸ“Š Result length:', result.length, 'characters');
        
        if (!result && !hasReceivedSpeech) {
          reject(new Error('æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•'));
        } else if (!result && hasReceivedSpeech) {
          // If we received speech but no final result, return a message
          resolve('è¯­éŸ³è¯†åˆ«å®Œæˆï¼Œä½†æœªèƒ½è·å–å®Œæ•´ç»“æœï¼Œè¯·é‡è¯•');
        } else {
          resolve(result);
        }
      };

      this.recognition.onerror = (event) => {
        this.isListening = false;
        if (timeoutId) {
          clearTimeout(timeoutId);
        }
        
        console.error('Speech recognition error:', event.error);
        
        let errorMessage = 'è¯­éŸ³è¯†åˆ«å¤±è´¥';
        switch (event.error) {
          case 'not-allowed':
            errorMessage = 'è¯·å…è®¸è®¿é—®éº¦å…‹é£ä»¥ä½¿ç”¨è¯­éŸ³è¯†åˆ«åŠŸèƒ½';
            break;
          case 'no-speech':
            errorMessage = 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•';
            break;
          case 'network':
            errorMessage = 'ç½‘ç»œé”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥';
            break;
          case 'audio-capture':
            errorMessage = 'éŸ³é¢‘æ•è·å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£';
            break;
          case 'aborted':
            // Don't treat manual abort as an error
            if (hasReceivedSpeech) {
              resolve(finalTranscript.trim());
              return;
            }
            errorMessage = 'è¯­éŸ³è¯†åˆ«å·²å–æ¶ˆ';
            break;
        }
        
        reject(new Error(errorMessage));
      };

      try {
        this.recognition.start();
      } catch (error) {
        this.isListening = false;
        reject(error);
      }
    });
  }

  stopListening(): void {
    if (this.recognition && this.isListening) {
      this.recognition.stop();
    }
  }

  getListeningState(): boolean {
    return this.isListening;
  }

  // Test microphone access and functionality
  async testMicrophone(): Promise<{
    hasPermission: boolean;
    canRecord: boolean;
    message: string;
  }> {
    try {
      // Test 1: Check if Web Speech API is supported
      if (!this.recognition) {
        return {
          hasPermission: false,
          canRecord: false,
          message: 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨ Chrome æˆ– Edge æµè§ˆå™¨'
        };
      }

      // Test 2: Check microphone permission
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      
      // Test 3: Quick speech recognition test
      return new Promise((resolve) => {
        if (!this.recognition) {
          resolve({
            hasPermission: true,
            canRecord: false,
            message: 'è¯­éŸ³è¯†åˆ«æœåŠ¡æœªåˆå§‹åŒ–'
          });
          return;
        }

        let testCompleted = false;
        const testTimeout = setTimeout(() => {
          if (!testCompleted) {
            testCompleted = true;
            this.recognition?.stop();
            resolve({
              hasPermission: true,
              canRecord: true,
              message: 'éº¦å…‹é£æµ‹è¯•æˆåŠŸï¼æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨è¯­éŸ³è¾“å…¥åŠŸèƒ½ã€‚'
            });
          }
        }, 3000); // 3 second test

        this.recognition.onstart = () => {
          console.log('Microphone test: Speech recognition started');
        };

        this.recognition.onresult = (event) => {
          if (!testCompleted) {
            testCompleted = true;
            clearTimeout(testTimeout);
            this.recognition?.stop();
            
            // Clean up the stream
            stream.getTracks().forEach(track => track.stop());
            
            resolve({
              hasPermission: true,
              canRecord: true,
              message: 'éº¦å…‹é£æµ‹è¯•æˆåŠŸï¼æ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥ï¼Œç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚'
            });
          }
        };

        this.recognition.onerror = (event) => {
          if (!testCompleted) {
            testCompleted = true;
            clearTimeout(testTimeout);
            
            // Clean up the stream
            stream.getTracks().forEach(track => track.stop());
            
            let message = 'éº¦å…‹é£æµ‹è¯•å¤±è´¥ï¼š';
            switch (event.error) {
              case 'not-allowed':
                message += 'è¯·å…è®¸è®¿é—®éº¦å…‹é£æƒé™';
                break;
              case 'no-speech':
                message += 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œä½†éº¦å…‹é£æƒé™æ­£å¸¸';
                break;
              case 'audio-capture':
                message += 'éŸ³é¢‘æ•è·å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£è®¾å¤‡';
                break;
              case 'network':
                message += 'ç½‘ç»œè¿æ¥é—®é¢˜';
                break;
              default:
                message += event.error;
            }
            
            resolve({
              hasPermission: true,
              canRecord: event.error !== 'not-allowed' && event.error !== 'audio-capture',
              message
            });
          }
        };

        this.recognition.onend = () => {
          if (!testCompleted) {
            testCompleted = true;
            clearTimeout(testTimeout);
            
            // Clean up the stream
            stream.getTracks().forEach(track => track.stop());
            
            resolve({
              hasPermission: true,
              canRecord: true,
              message: 'éº¦å…‹é£æµ‹è¯•å®Œæˆï¼æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œä½†è®¾å¤‡å·¥ä½œæ­£å¸¸ã€‚'
            });
          }
        };

        try {
          this.recognition.start();
        } catch (error) {
          testCompleted = true;
          clearTimeout(testTimeout);
          
          // Clean up the stream
          stream.getTracks().forEach(track => track.stop());
          
          resolve({
            hasPermission: true,
            canRecord: false,
            message: `è¯­éŸ³è¯†åˆ«å¯åŠ¨å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
          });
        }
      });

    } catch (error) {
      console.error('Microphone test error:', error);
      
      if (error instanceof Error) {
        if (error.name === 'NotAllowedError') {
          return {
            hasPermission: false,
            canRecord: false,
            message: 'éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸è®¿é—®éº¦å…‹é£'
          };
        } else if (error.name === 'NotFoundError') {
          return {
            hasPermission: false,
            canRecord: false,
            message: 'æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æ˜¯å¦æ­£ç¡®è¿æ¥'
          };
        } else if (error.name === 'NotSupportedError') {
          return {
            hasPermission: false,
            canRecord: false,
            message: 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒéº¦å…‹é£è®¿é—®åŠŸèƒ½'
          };
        }
      }
      
      return {
        hasPermission: false,
        canRecord: false,
        message: `éº¦å…‹é£æµ‹è¯•å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
      };
    }
  }
}

// Export singleton instances
export const voiceRecordingService = new VoiceRecordingService();
export const webSpeechService = new WebSpeechService();